{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024bbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\bartm\\\\Documents\\\\These\\\\phyloreplica\\\\src')\n",
    "from PhyloDataset import *\n",
    "from PhyloTrees import *\n",
    "from vae import *\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datapath1 = \"../data/PF00072/PF00072_rp15_has_PF00196.faa\"\n",
    "datapath2 = \"../data/PF00072/PF00072_rp15_has_PF00486.faa\"\n",
    "datapath3 = \"../data/PF00072/PF00072_rp15_has_PF00512.faa\"\n",
    "datapath4 = \"data/PF00072/PF00072_rp15_has_PF00158.faa\"\n",
    "datapath5 = \"data/PF00072/PF00072_rp15_has_PF00990.faa\"\n",
    "datapath6 = \"data/PF00072/PF00072_rp15_has_PF01339.faa\"\n",
    "datapath7 = \"data/PF00072/PF00072_rp15_has_PF04397.faa\"\n",
    "datapath8 = \"data/PF00072/PF00072_rp15_has_PF12833.faa\"\n",
    "\n",
    "lossfn = vae_loss\n",
    "\n",
    "dataset1 = MSA(datapath1)\n",
    "dataset2 = MSA(datapath2)\n",
    "dataset3 = MSA(datapath3)\n",
    "lt = len(dataset1) + len(dataset2) + len(dataset1)\n",
    "l1 = int(32*len(dataset1)/lt) \n",
    "l2 = int(32*len(dataset2)/lt) \n",
    "l3 = 32 - l1 -l2\n",
    "\n",
    "vae1 = VAE(21, 5, dataset1.len_protein * dataset1.q, [512, 256, 128])\n",
    "optimizer1 = optim.Adam(vae1.parameters(),weight_decay=0.01)\n",
    "Node1 = PhyloNode(vae1,\n",
    "          optimizer1, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset1, \n",
    "          tuplesize=2, \n",
    "          batch_size=l1, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name = \"196\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae2 = VAE(21, 5, dataset2.len_protein * dataset2.q, [512, 256, 128])\n",
    "optimizer2 = optim.Adam(vae2.parameters(),weight_decay=0.01)\n",
    "Node2 = PhyloNode(vae2,\n",
    "          optimizer2, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset2, \n",
    "          tuplesize=2, \n",
    "          batch_size=l2, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"486\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae3 = VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizer3 = optim.Adam(vae3.parameters(),weight_decay=0.01)\n",
    "Node3 = PhyloNode(vae3,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset3, \n",
    "          tuplesize=2, \n",
    "          batch_size=l3, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"512\"\n",
    "    )\n",
    "\n",
    "vaeR =  VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizerR = optim.Adam(vaeR.parameters(),weight_decay=0.01)\n",
    "NodeR = PhyloNode(vaeR,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset3, \n",
    "          tuplesize=2, \n",
    "          batch_size=32, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"Root\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db740d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1414dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer1, factor=0.5, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5764e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainL = int(0.8 * len(dataset1))\n",
    "testL = int(0.1 * len(dataset1))\n",
    "valL = len(dataset1) - trainL -testL\n",
    "batch_size = 64\n",
    "train_set, test_set,  val_set = torch.utils.data.random_split(dataset1, [trainL, testL, valL])\n",
    "train_iterator = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_iterator = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "val_iterator = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " acc train 204.53567902907514\n",
      " acc val 205.4270546166579\n",
      "1\n",
      " acc train 203.50727385637046\n",
      " acc val 204.14234303127202\n",
      "2\n",
      " acc train 202.26023094538542\n",
      " acc val 202.00684154112255\n",
      "3\n",
      " acc train 200.65438359120253\n",
      " acc val 200.69545820363027\n",
      "4\n",
      " acc train 198.97053537566242\n",
      " acc val 199.8662372129823\n",
      "5\n",
      " acc train 197.6810588367946\n",
      " acc val 198.3893510942112\n",
      "6\n",
      " acc train 196.7912746605711\n",
      " acc val 197.97041772420317\n",
      "7\n",
      " acc train 197.12000389956597\n",
      " acc val 198.45399415136487\n",
      "8\n",
      " acc train 195.82646905048833\n",
      " acc val 196.7679419281017\n",
      "9\n",
      " acc train 194.65443221147336\n",
      " acc val 196.73703722525394\n",
      "10\n",
      " acc train 193.73209173865354\n",
      " acc val 196.8974296206834\n",
      "11\n",
      " acc train 194.18732172662854\n",
      " acc val 196.98742779726157\n",
      "12\n",
      " acc train 193.26308507306092\n",
      " acc val 195.3205991268063\n",
      "13\n",
      " acc train 192.52886123866023\n",
      " acc val 195.76202529664664\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(epoch)\n",
    "    vae1.train()\n",
    "    train_loss_list = []\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        x,w = batch[0], batch[1]\n",
    "        #targ = targ.unsqueeze(1)\n",
    "        loss = -vae1.compute_weighted_elbo(x, w)\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "    \n",
    "#         l2_reg = torch.tensor(0.)\n",
    "#         for param in Dcla.parameters():\n",
    "#             l2_reg += torch.norm(param)\n",
    "#         loss += l2_lambda * l2_reg\n",
    "\n",
    "    print(\" acc train\" ,np.mean(train_loss_list))\n",
    "#     scheduler.step(np.mean(train_loss_list))\n",
    "    test_loss_list = []\n",
    "    with  torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_iterator):\n",
    "            x,w = batch[0], batch[1]\n",
    "            loss = -vae1.compute_weighted_elbo(x, w)\n",
    "            test_loss_list.append(loss.item())\n",
    "    print(\" acc val\" ,np.mean(test_loss_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615943c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
