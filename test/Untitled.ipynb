{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53aa977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\bartm\\\\Documents\\\\These\\\\phyloreplica\\\\src')\n",
    "from PhyloDataset import *\n",
    "from PhyloTrees import *\n",
    "from vae import *\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datapath1 = \"../data/PF00072/PF00072_rp15_has_PF00196.faa\"\n",
    "datapath2 = \"../data/PF00072/PF00072_rp15_has_PF00486.faa\"\n",
    "datapath3 = \"../data/PF00072/PF00072_rp15_has_PF00512.faa\"\n",
    "datapath4 = \"data/PF00072/PF00072_rp15_has_PF00158.faa\"\n",
    "datapath5 = \"data/PF00072/PF00072_rp15_has_PF00990.faa\"\n",
    "datapath6 = \"data/PF00072/PF00072_rp15_has_PF01339.faa\"\n",
    "datapath7 = \"data/PF00072/PF00072_rp15_has_PF04397.faa\"\n",
    "datapath8 = \"data/PF00072/PF00072_rp15_has_PF12833.faa\"\n",
    "\n",
    "lossfn = vae_loss\n",
    "\n",
    "dataset1 = MSA(datapath1)\n",
    "dataset2 = MSA(datapath2)\n",
    "dataset3 = MSA(datapath3)\n",
    "lt = len(dataset1) + len(dataset2) + len(dataset1)\n",
    "l1 = int(32*len(dataset1)/lt) \n",
    "l2 = int(32*len(dataset2)/lt) \n",
    "l3 = 32 - l1 -l2\n",
    "\n",
    "vae1 = VAE(21, 5, dataset1.len_protein * dataset1.q, [512, 256, 128])\n",
    "optimizer1 = optim.Adam(vae1.parameters(),weight_decay=0.01)\n",
    "Node1 = PhyloNode(vae1,\n",
    "          optimizer1, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset1, \n",
    "          tuplesize=2, \n",
    "          batch_size=l1, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name = \"196\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae2 = VAE(21, 5, dataset2.len_protein * dataset2.q, [512, 256, 128])\n",
    "optimizer2 = optim.Adam(vae2.parameters(),weight_decay=0.01)\n",
    "Node2 = PhyloNode(vae2,\n",
    "          optimizer2, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset2, \n",
    "          tuplesize=2, \n",
    "          batch_size=l2, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"486\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae3 = VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizer3 = optim.Adam(vae3.parameters(),weight_decay=0.01)\n",
    "Node3 = PhyloNode(vae3,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset3, \n",
    "          tuplesize=2, \n",
    "          batch_size=l3, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"512\"\n",
    "    )\n",
    "\n",
    "vaeR =  VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizerR = optim.Adam(vaeR.parameters(),weight_decay=0.01)\n",
    "NodeR = PhyloNode(vaeR,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset3, \n",
    "          tuplesize=2, \n",
    "          batch_size=32, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"Root\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d29f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac82e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer1, factor=0.5, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f96ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainL = int(0.8 * len(dataset1))\n",
    "testL = int(0.1 * len(dataset1))\n",
    "valL = len(dataset1) - trainL -testL\n",
    "batch_size = 64\n",
    "train_set, test_set,  val_set = torch.utils.data.random_split(dataset1, [trainL, testL, valL])\n",
    "train_iterator = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_iterator = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "val_iterator = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fdb5701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " acc train 204.53567902907514\n",
      " acc val 205.4270546166579\n",
      "1\n",
      " acc train 203.50727385637046\n",
      " acc val 204.14234303127202\n",
      "2\n",
      " acc train 202.26023094538542\n",
      " acc val 202.00684154112255\n",
      "3\n",
      " acc train 200.65438359120253\n",
      " acc val 200.69545820363027\n",
      "4\n",
      " acc train 198.97053537566242\n",
      " acc val 199.8662372129823\n",
      "5\n",
      " acc train 197.6810588367946\n",
      " acc val 198.3893510942112\n",
      "6\n",
      " acc train 196.7912746605711\n",
      " acc val 197.97041772420317\n",
      "7\n",
      " acc train 197.12000389956597\n",
      " acc val 198.45399415136487\n",
      "8\n",
      " acc train 195.82646905048833\n",
      " acc val 196.7679419281017\n",
      "9\n",
      " acc train 194.65443221147336\n",
      " acc val 196.73703722525394\n",
      "10\n",
      " acc train 193.73209173865354\n",
      " acc val 196.8974296206834\n",
      "11\n",
      " acc train 194.18732172662854\n",
      " acc val 196.98742779726157\n",
      "12\n",
      " acc train 193.26308507306092\n",
      " acc val 195.3205991268063\n",
      "13\n",
      " acc train 192.52886123866023\n",
      " acc val 195.76202529664664\n",
      "14\n",
      " acc train 192.39569608933564\n",
      " acc val 194.31402398995502\n",
      "15\n",
      " acc train 191.67377383367028\n",
      " acc val 194.04485889730023\n",
      "16\n",
      " acc train 190.9551520296777\n",
      " acc val 194.50466177295462\n",
      "17\n",
      " acc train 191.06229127094994\n",
      " acc val 193.8260238963091\n",
      "18\n",
      " acc train 190.99566108230317\n",
      " acc val 193.30473758175387\n",
      "19\n",
      " acc train 190.76283494583916\n",
      " acc val 193.9498132005588\n",
      "20\n",
      " acc train 190.5744679468559\n",
      " acc val 193.44835118789507\n",
      "21\n",
      " acc train 190.71777638054536\n",
      " acc val 194.73084526778536\n",
      "22\n",
      " acc train 191.46749025042257\n",
      " acc val 193.7939728544308\n",
      "23\n",
      " acc train 189.9249624234412\n",
      " acc val 193.75414413709808\n",
      "24\n",
      " acc train 189.63439871303504\n",
      " acc val 193.53774138468137\n",
      "25\n",
      " acc train 189.22701002662274\n",
      " acc val 192.14510864700586\n",
      "26\n",
      " acc train 189.25556082636248\n",
      " acc val 192.28705157589366\n",
      "27\n",
      " acc train 188.7810138404882\n",
      " acc val 193.0232928941823\n",
      "28\n",
      " acc train 188.58414220367965\n",
      " acc val 192.21987418859715\n",
      "29\n",
      " acc train 188.20436455430627\n",
      " acc val 192.50136805429486\n",
      "30\n",
      " acc train 188.0020867631651\n",
      " acc val 190.8906658657863\n",
      "31\n",
      " acc train 187.8194215582191\n",
      " acc val 192.77570595929726\n",
      "32\n",
      " acc train 188.30537485708845\n",
      " acc val 194.28975123032257\n",
      "33\n",
      " acc train 188.56865689835908\n",
      " acc val 192.79170170119568\n",
      "34\n",
      " acc train 187.75197867349615\n",
      " acc val 192.43397941960322\n",
      "35\n",
      " acc train 187.8715412720353\n",
      " acc val 191.82892688520704\n",
      "36\n",
      " acc train 187.42481394670554\n",
      " acc val 190.6768315789207\n",
      "37\n",
      " acc train 187.38500432559692\n",
      " acc val 191.45220125019813\n",
      "38\n",
      " acc train 187.29981013284834\n",
      " acc val 191.29683596403382\n",
      "39\n",
      " acc train 187.35133139861426\n",
      " acc val 191.19635048156954\n",
      "40\n",
      " acc train 187.23246472299536\n",
      " acc val 193.61480226743708\n",
      "41\n",
      " acc train 187.52986714607178\n",
      " acc val 190.94326614671405\n",
      "42\n",
      " acc train 186.8356304135372\n",
      " acc val 190.23923174008962\n",
      "43\n",
      " acc train 186.6788443496356\n",
      " acc val 191.21857731803223\n",
      "44\n",
      " acc train 186.3522583365712\n",
      " acc val 191.15031971233427\n",
      "45\n",
      " acc train 186.16753240624195\n",
      " acc val 191.3912798741333\n",
      "46\n",
      " acc train 186.19228842437005\n",
      " acc val 190.88063954989445\n",
      "47\n",
      " acc train 185.90764901464047\n",
      " acc val 189.93148811258558\n",
      "48\n",
      " acc train 185.76030440778518\n",
      " acc val 190.6085649460529\n",
      "49\n",
      " acc train 186.0173704986346\n",
      " acc val 190.74344569207102\n",
      "50\n",
      " acc train 186.9032623951644\n",
      " acc val 192.74782849923668\n",
      "51\n",
      " acc train 186.12741451684326\n",
      " acc val 190.7418676371192\n",
      "52\n",
      " acc train 185.74216447491915\n",
      " acc val 189.97267358830172\n",
      "53\n",
      " acc train 185.0814075469565\n",
      " acc val 190.6078397390605\n",
      "54\n",
      " acc train 185.62142130856935\n",
      " acc val 190.9901117615935\n",
      "55\n",
      " acc train 185.62965795766024\n",
      " acc val 189.95698962118678\n",
      "56\n",
      " acc train 185.3891337526873\n",
      " acc val 191.11038339015957\n",
      "57\n",
      " acc train 184.9826811556088\n",
      " acc val 190.31742637016663\n",
      "58\n",
      " acc train 184.79100474972404\n",
      " acc val 189.94015374790914\n",
      "59\n",
      " acc train 184.5990446782611\n",
      " acc val 190.98294013718836\n",
      "60\n",
      " acc train 185.32721709726584\n",
      " acc val 190.37423968468298\n",
      "61\n",
      " acc train 184.384174777252\n",
      " acc val 190.3628599957682\n",
      "62\n",
      " acc train 184.48745866661088\n",
      " acc val 189.89453359591465\n",
      "63\n",
      " acc train 184.5244621280538\n",
      " acc val 190.03089088677547\n",
      "64\n",
      " acc train 184.40688272572845\n",
      " acc val 190.8766936352376\n",
      "65\n",
      " acc train 184.72801639604901\n",
      " acc val 190.93534634412984\n",
      "66\n",
      " acc train 184.4353839049464\n",
      " acc val 189.0997614570028\n",
      "67\n",
      " acc train 184.49795682402578\n",
      " acc val 189.5282555217656\n",
      "68\n",
      " acc train 183.65497841982523\n",
      " acc val 189.5998506699953\n",
      "69\n",
      " acc train 184.05072692963458\n",
      " acc val 189.46147356208783\n",
      "70\n",
      " acc train 185.117239068862\n",
      " acc val 190.8665748936445\n",
      "71\n",
      " acc train 185.18182768571668\n",
      " acc val 189.32396751800255\n",
      "72\n",
      " acc train 184.6359569386646\n",
      " acc val 189.89502402638215\n",
      "73\n",
      " acc train 183.762370939411\n",
      " acc val 189.7732882227883\n",
      "74\n",
      " acc train 183.41142013198186\n",
      " acc val 189.73781415501895\n",
      "75\n",
      " acc train 183.39379351111813\n",
      " acc val 189.7893244245239\n",
      "76\n",
      " acc train 183.32655832241934\n",
      " acc val 189.80981636424505\n",
      "77\n",
      " acc train 183.01677797426416\n",
      " acc val 189.21976830689604\n",
      "78\n",
      " acc train 183.03052497444358\n",
      " acc val 188.94895965501505\n",
      "79\n",
      " acc train 182.75455681908937\n",
      " acc val 188.67977482725402\n",
      "80\n",
      " acc train 182.8411936595719\n",
      " acc val 189.670757631272\n",
      "81\n",
      " acc train 182.6959055066718\n",
      " acc val 189.2264947291537\n",
      "82\n",
      " acc train 182.77536836282167\n",
      " acc val 189.92856195501676\n",
      "83\n",
      " acc train 182.52668819587066\n",
      " acc val 189.03020237577218\n",
      "84\n",
      " acc train 182.66480504607668\n",
      " acc val 189.38469049339983\n",
      "85\n",
      " acc train 182.7930682927905\n",
      " acc val 189.15761869284194\n",
      "86\n",
      " acc train 182.61730958738252\n",
      " acc val 189.49027055472172\n",
      "87\n",
      " acc train 183.44139082176625\n",
      " acc val 190.40558609128146\n",
      "88\n",
      " acc train 183.29336654588013\n",
      " acc val 189.6127350079403\n",
      "89\n",
      " acc train 182.589096197695\n",
      " acc val 189.2287148519192\n",
      "90\n",
      " acc train 182.00304696292906\n",
      " acc val 188.78925684259022\n",
      "91\n",
      " acc train 181.94422110175088\n",
      " acc val 188.8371772045812\n",
      "92\n",
      " acc train 181.7090325334655\n",
      " acc val 188.87369866949044\n",
      "93\n",
      " acc train 181.64904593869\n",
      " acc val 188.9479255921238\n",
      "94\n",
      " acc train 181.74020264854025\n",
      " acc val 189.16752829513428\n",
      "95\n",
      " acc train 181.5824845887838\n",
      " acc val 188.25100127068225\n",
      "96\n",
      " acc train 181.48603654012604\n",
      " acc val 189.27846250979914\n",
      "97\n",
      " acc train 181.50402300347736\n",
      " acc val 188.69465355073498\n",
      "98\n",
      " acc train 181.41781947735643\n",
      " acc val 188.91174288638706\n",
      "99\n",
      " acc train 181.50121912448566\n",
      " acc val 189.4383974866998\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(epoch)\n",
    "    vae1.train()\n",
    "    train_loss_list = []\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        x,w = batch[0], batch[1]\n",
    "        #targ = targ.unsqueeze(1)\n",
    "        loss = -vae1.compute_weighted_elbo(x, w)\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "    \n",
    "#         l2_reg = torch.tensor(0.)\n",
    "#         for param in Dcla.parameters():\n",
    "#             l2_reg += torch.norm(param)\n",
    "#         loss += l2_lambda * l2_reg\n",
    "\n",
    "    print(\" acc train\" ,np.mean(train_loss_list))\n",
    "#     scheduler.step(np.mean(train_loss_list))\n",
    "    test_loss_list = []\n",
    "    with  torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_iterator):\n",
    "            x,w = batch[0], batch[1]\n",
    "            loss = -vae1.compute_weighted_elbo(x, w)\n",
    "            test_loss_list.append(loss.item())\n",
    "    print(\" acc val\" ,np.mean(test_loss_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f7a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
