{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce26f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\bartm\\\\Documents\\\\These\\\\phyloreplica\\\\src')\n",
    "from PhyloDataset import *\n",
    "from PhyloTrees import *\n",
    "\n",
    "from vae import *\n",
    "# from GProT import *\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datapath1 = \"../data/PF00072/PF00072_rp15_has_PF00196.faa\"\n",
    "datapath2 = \"../data/PF00072/PF00072_rp15_has_PF00486.faa\"\n",
    "datapath3 = \"../data/PF00072/PF00072_rp15_has_PF00512.faa\"\n",
    "datapath4 = \"data/PF00072/PF00072_rp15_has_PF00158.faa\"\n",
    "datapath5 = \"data/PF00072/PF00072_rp15_has_PF00990.faa\"\n",
    "datapath6 = \"data/PF00072/PF00072_rp15_has_PF01339.faa\"\n",
    "datapath7 = \"data/PF00072/PF00072_rp15_has_PF04397.faa\"\n",
    "datapath8 = \"data/PF00072/PF00072_rp15_has_PF12833.faa\"\n",
    "\n",
    "lossfn = vae_loss\n",
    "\n",
    "dataset1 = MSA(datapath1,onehot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a51c59",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_828/3068804825.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\bartm\\AppData\\Local\\Temp/ipykernel_828/3068804825.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    0::2\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0::2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    source: https://github.com/pytorch/tutorials/blob/011ae8a6d47a960935d0401acda71d0e400088d6/advanced_source/ddp_pipeline.py#L43\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=25,device='cpu'):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.device = device\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)[:,:pe[:, 0::2].shape[1]]\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:,:pe[:, 1::2].shape[1]]\n",
    "        #pe = pe[:,:-1] #this step is not really smart, needed when d_model is odd. \n",
    "        pe = pe.unsqueeze(0)#.transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        print(self.pe.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        #print( self.pe[:x.size(0), :].shape)\n",
    "        \n",
    "        x = (x + self.pe[:, :x.size(1)]).to(self.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        # print(\"attshape\", att.shape)\n",
    "        # print(att[0,0,:10,:10])\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        \n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        print(\"yshape\", y.shape)\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)#n_embd\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)#n_embd\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4*config.n_embd),#n_embd-4n_embd\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*config.n_embd, config.n_embd),#4n_embd-n_embd\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        # self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.position_embedding = PositionalEncoding(config.n_embd, max_len=config.block_size, device=device)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        self.device = device\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)#n_embd\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)#n_embd#n_embd\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, seq, targets=None):\n",
    "        b, L= seq.size()\n",
    "        assert L <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "        print(L)\n",
    "        # # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(seq.long()) # each index maps to a (learnable) vector\n",
    "        # position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "\n",
    "        x = self.position_embedding(token_embeddings) #self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GPT_loss(model, batch):\n",
    "\n",
    "    inp = model(batch[0][:,:-1])[0].reshape(-1, model.vocab_size)\n",
    "\n",
    "    tar = batch[0][:,1:].flatten().long()\n",
    "\n",
    "    \n",
    "    return F.cross_entropy(inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04afb714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (12) must match the existing size (13) at non-singleton dimension 1.  Target sizes: [25, 12].  Tensor sizes: [25, 13]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_828/1509438913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0md_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[0mPositionalEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_828/1509438913.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, d_model, dropout, max_len, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdiv_term\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m#pe = pe[:,:-1] #this step is not really smart, needed when d_model is odd.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.transpose(0, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (12) must match the existing size (13) at non-singleton dimension 1.  Target sizes: [25, 12].  Tensor sizes: [25, 13]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "d_model = 25\n",
    "PositionalEncoding(d_model, dropout=0.1, max_len=25,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74f407ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 1])\n",
      "torch.Size([13])\n",
      "torch.Size([99, 13])\n",
      "torch.Size([99, 26]) torch.Size([99, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  4.7273e-01,  ...,  1.0000e+00,\n",
       "           2.0309e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  8.3315e-01,  ...,  1.0000e+00,\n",
       "           4.0618e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 9.8359e-01, -1.8043e-01, -1.4487e-01,  ...,  9.9922e-01,\n",
       "           1.9496e-02,  9.9981e-01],\n",
       "         [ 3.7961e-01, -9.2515e-01, -5.9541e-01,  ...,  9.9920e-01,\n",
       "           1.9699e-02,  9.9981e-01],\n",
       "         [-5.7338e-01, -8.1929e-01, -9.0448e-01,  ...,  9.9918e-01,\n",
       "           1.9902e-02,  9.9980e-01]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 99\n",
    "d_model = 26\n",
    "device = \"cpu\"\n",
    "pe = torch.zeros(max_len, d_model).to(device)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "print(position.shape)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "print(div_term.shape)\n",
    "print((position * div_term).shape)\n",
    "pe[:, 0::2] = torch.sin(position * div_term)[:,:pe[:, 0::2].shape[1]]\n",
    "print(pe.shape, pe[:, 1::2].shape)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)[:,:pe[:, 1::2].shape[1]]\n",
    "#pe = pe[:,:-1] #this step is not really smart, needed when d_model is odd. \n",
    "pe = pe.unsqueeze(0)#.transpose(0, 1)\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b45c9742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 1])\n",
      "torch.Size([13])\n",
      "torch.Size([99, 13])\n",
      "torch.Size([99, 26]) torch.Size([99, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  4.7273e-01,  ...,  1.0000e+00,\n",
       "           2.0309e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  8.3315e-01,  ...,  1.0000e+00,\n",
       "           4.0618e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 9.8359e-01, -1.8043e-01, -1.4487e-01,  ...,  9.9922e-01,\n",
       "           1.9496e-02,  9.9981e-01],\n",
       "         [ 3.7961e-01, -9.2515e-01, -5.9541e-01,  ...,  9.9920e-01,\n",
       "           1.9699e-02,  9.9981e-01],\n",
       "         [-5.7338e-01, -8.1929e-01, -9.0448e-01,  ...,  9.9918e-01,\n",
       "           1.9902e-02,  9.9980e-01]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 99\n",
    "d_model = 26\n",
    "device = \"cpu\"\n",
    "pee = torch.zeros(max_len, d_model).to(device)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "print(position.shape)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "print(div_term.shape)\n",
    "print((position * div_term).shape)\n",
    "pee[:, 0::2] = torch.sin(position * div_term)\n",
    "print(pee.shape, pee[:, 1::2].shape)\n",
    "pee[:, 1::2] = torch.cos(position * div_term)\n",
    "#pee = pee[:,:-1] #this step is not really smart, needed when d_model is odd. \n",
    "pee = pee.unsqueeze(0)#.transpose(0, 1)\n",
    "pee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed0eeb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(pe,pee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5b67efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24])\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24])\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23])\n",
      "torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "d_model = 25\n",
    "a = torch.tensor(range(d_model))\n",
    "print(a)\n",
    "print(a[0::2])\n",
    "print(a[1::2])\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "print(div_term.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 25\n",
    "\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3ae66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(21, 112)\n",
    "co = GPT1Config(21, 112)\n",
    "co.n_layer = 1\n",
    "co.n_head = 1\n",
    "co.n_embd = 50*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d76c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(co, dataset1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabce100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(dataset1, batch_size=32, shuffle=True)\n",
    "batch = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f5dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.Adam(model.parameters())\n",
    "\n",
    "for batchidx, batch in enumerate(train_iterator):\n",
    "    optimizer1.zero_grad()\n",
    "    loss = GPT_loss(model, batch)\n",
    "    loss.backward()\n",
    "    optimizer1.step()\n",
    "    print(loss)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
