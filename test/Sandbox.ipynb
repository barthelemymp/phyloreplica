{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ef30ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\bartm\\\\Documents\\\\These\\\\phyloreplica\\\\src')\n",
    "from PhyloDataset import *\n",
    "\n",
    "from PhyloTrees import *\n",
    "\n",
    "from vae import *\n",
    "\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datapath1 = \"../data/PF00072/PF00072_rp15_has_PF00196.faa\"\n",
    "datapath2 = \"../data/PF00072/PF00072_rp15_has_PF00486.faa\"\n",
    "datapath3 = \"../data/PF00072/PF00072_rp15_has_PF00512.faa\"\n",
    "datapath4 = \"data/PF00072/PF00072_rp15_has_PF00158.faa\"\n",
    "datapath5 = \"data/PF00072/PF00072_rp15_has_PF00990.faa\"\n",
    "datapath6 = \"data/PF00072/PF00072_rp15_has_PF01339.faa\"\n",
    "datapath7 = \"data/PF00072/PF00072_rp15_has_PF04397.faa\"\n",
    "datapath8 = \"data/PF00072/PF00072_rp15_has_PF12833.faa\"\n",
    "\n",
    "lossfn = vae_loss\n",
    "\n",
    "dataset1 = MSA(datapath1)\n",
    "dataset2 = MSA(datapath2)\n",
    "dataset3 = MSA(datapath3)\n",
    "lt = len(dataset1) + len(dataset2) + len(dataset1)\n",
    "l1 = int(32*len(dataset1)/lt) \n",
    "l2 = int(32*len(dataset2)/lt) \n",
    "l3 = 32 - l1 -l2\n",
    "\n",
    "vae1 = VAE(21, 5, dataset1.len_protein * dataset1.q, [512, 256, 128])\n",
    "optimizer1 = optim.Adam(vae1.parameters(),weight_decay=0.01)\n",
    "Node1 = PhyloNode(vae1,\n",
    "          optimizer1, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset1, \n",
    "          tuplesize=2, \n",
    "          batch_size=l1, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name = \"196\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae2 = VAE(21, 5, dataset2.len_protein * dataset2.q, [512, 256, 128])\n",
    "optimizer2 = optim.Adam(vae2.parameters(),weight_decay=0.01)\n",
    "Node2 = PhyloNode(vae2,\n",
    "          optimizer2, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset2, \n",
    "          tuplesize=2, \n",
    "          batch_size=l2, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"486\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae3 = VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizer3 = optim.Adam(vae3.parameters(),weight_decay=0.01)\n",
    "Node3 = PhyloNode(vae3,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset3, \n",
    "          tuplesize=2, \n",
    "          batch_size=l3, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"512\"\n",
    "    )\n",
    "\n",
    "vaeR =  VAE(21, 5, dataset3.len_protein * dataset3.q, [512, 256, 128])\n",
    "optimizerR = optim.Adam(vaeR.parameters(),weight_decay=0.01)\n",
    "NodeR = PhyloNode(vaeR,\n",
    "          optimizer3, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          tuplesize=2, \n",
    "          batch_size=32, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"Root\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae4060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NodeR.addChildren(Node1)\n",
    "NodeR.addChildren(Node2)\n",
    "NodeR.addChildren(Node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fd90656",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8537.2959, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recursive=True\n",
    "NodeR.getNewTrainBatch(fullBatch=False)\n",
    "NodeR.trainmode(recursive=recursive)\n",
    "NodeR.computeLoss(recursive=recursive)\n",
    "NodeR.computeCouplingLossChildren(recursive=recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f223f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8537.2959, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NodeR.model.encoder_linears[0].weight\n",
    "NodeR.coupling_loss_Children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f55df463",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NodeR.to_(device, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaebd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer1, factor=0.5, patience=10, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bead82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainL = int(0.8 * len(dataset1))\n",
    "testL = int(0.1 * len(dataset1))\n",
    "valL = len(dataset1) - trainL -testL\n",
    "batch_size = 64\n",
    "train_set, test_set,  val_set = torch.utils.data.random_split(dataset1, [trainL, testL, valL])\n",
    "train_iterator = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_iterator = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "val_iterator = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a68a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " acc train 232.64005801213642\n",
      " acc val 216.81131757967438\n",
      "1\n",
      " acc train 221.08789975881538\n",
      " acc val 216.8724232900696\n",
      "2\n",
      " acc train 220.78262209176722\n",
      " acc val 216.28017581379268\n",
      "3\n",
      " acc train 221.00017979834374\n",
      " acc val 216.22171973935576\n",
      "4\n",
      " acc train 218.22384155384952\n",
      " acc val 210.8948803907571\n",
      "5\n",
      " acc train 213.40594894706072\n",
      " acc val 207.86210863053896\n",
      "6\n",
      " acc train 209.83776014020359\n",
      " acc val 206.54911125779657\n",
      "7\n",
      " acc train 207.53771635573955\n",
      " acc val 204.90933909563287\n",
      "8\n",
      " acc train 206.46169362608848\n",
      " acc val 203.47775012884782\n",
      "9\n",
      " acc train 204.993235802025\n",
      " acc val 201.76293855947455\n",
      "10\n",
      " acc train 203.13573688453093\n",
      " acc val 201.08824371574417\n",
      "11\n",
      " acc train 201.41997401246047\n",
      " acc val 199.39542016606822\n",
      "12\n",
      " acc train 200.41088235460992\n",
      " acc val 198.73058363782368\n",
      "13\n",
      " acc train 199.7113907324935\n",
      " acc val 198.38729480262143\n",
      "14\n",
      " acc train 199.21865676814903\n",
      " acc val 198.65687004912672\n",
      "15\n",
      " acc train 198.80171879379918\n",
      " acc val 199.12513417776984\n",
      "16\n",
      " acc train 198.69112326924932\n",
      " acc val 198.20481054275717\n",
      "17\n",
      " acc train 197.96655551198242\n",
      " acc val 197.21889333467854\n",
      "18\n",
      " acc train 197.2519287525527\n",
      " acc val 197.80601983463396\n",
      "19\n",
      " acc train 197.09303406175638\n",
      " acc val 198.34413149056988\n",
      "20\n",
      " acc train 197.33392624505495\n",
      " acc val 197.41622099740783\n",
      "21\n",
      " acc train 198.1854255277486\n",
      " acc val 196.9991172813285\n",
      "22\n",
      " acc train 196.377726293388\n",
      " acc val 196.31990399618704\n",
      "23\n",
      " acc train 196.16646138302326\n",
      " acc val 196.12841916828822\n",
      "24\n",
      " acc train 195.73689674085978\n",
      " acc val 196.68772504010698\n",
      "25\n",
      " acc train 195.3404589410646\n",
      " acc val 196.50345905446045\n",
      "26\n",
      " acc train 195.30313441474854\n",
      " acc val 196.7563656206926\n",
      "27\n",
      " acc train 194.7931669928267\n",
      " acc val 196.0500568539175\n",
      "28\n",
      " acc train 194.81796372132465\n",
      " acc val 196.02054145969424\n",
      "29\n",
      " acc train 194.981201060162\n",
      " acc val 197.20320898352142\n",
      "30\n",
      " acc train 194.7450101441912\n",
      " acc val 195.11487765841554\n",
      "31\n",
      " acc train 195.2272310019333\n",
      " acc val 197.27753773484145\n",
      "32\n",
      " acc train 195.12413098303324\n",
      " acc val 196.06909172888683\n",
      "33\n",
      " acc train 195.14926846672762\n",
      " acc val 196.5307738133147\n",
      "34\n",
      " acc train 194.4254207824446\n",
      " acc val 196.33473425811343\n",
      "35\n",
      " acc train 194.1838890379554\n",
      " acc val 196.3183718368261\n",
      "36\n",
      " acc train 194.10697332920023\n",
      " acc val 196.50002120275155\n",
      "37\n",
      " acc train 193.8730939247243\n",
      " acc val 195.68859735464576\n",
      "38\n",
      " acc train 193.55124651273388\n",
      " acc val 194.4327506799075\n",
      "39\n",
      " acc train 193.24317415020147\n",
      " acc val 194.72711580041147\n",
      "40\n",
      " acc train 192.99614529187582\n",
      " acc val 195.41511121681808\n",
      "41\n",
      " acc train 192.96901204449915\n",
      " acc val 194.0437605371255\n",
      "42\n",
      " acc train 194.4838957655034\n",
      " acc val 194.17587276506433\n",
      "43\n",
      " acc train 192.95092883255802\n",
      " acc val 195.1861691393372\n",
      "44\n",
      " acc train 192.7904903941161\n",
      " acc val 194.22648929836487\n",
      "45\n",
      " acc train 191.71647952464303\n",
      " acc val 194.5685540774243\n",
      "46\n",
      " acc train 191.37807398492495\n",
      " acc val 194.2042898234292\n",
      "47\n",
      " acc train 191.88771048957295\n",
      " acc val 193.6051263593675\n",
      "48\n",
      " acc train 193.15202995455113\n",
      " acc val 193.02308734942034\n",
      "49\n",
      " acc train 191.12752880787386\n",
      " acc val 194.11589188850485\n",
      "50\n",
      " acc train 190.87263691317258\n",
      " acc val 192.73569155466598\n",
      "51\n",
      " acc train 190.4033920016098\n",
      " acc val 192.31021568471212\n",
      "52\n",
      " acc train 190.32105553270728\n",
      " acc val 192.91041596946286\n",
      "53\n",
      " acc train 190.08264736234264\n",
      " acc val 193.54367327755557\n",
      "54\n",
      " acc train 190.57038381736615\n",
      " acc val 193.83464946849728\n",
      "55\n",
      " acc train 189.5954770123568\n",
      " acc val 192.5231816328354\n",
      "56\n",
      " acc train 190.94430220938315\n",
      " acc val 192.11325137102628\n",
      "57\n",
      " acc train 188.75258171281007\n",
      " acc val 192.28341770340575\n",
      "58\n",
      " acc train 189.26526802124383\n",
      " acc val 193.9762152808091\n",
      "59\n",
      " acc train 188.48408236089608\n",
      " acc val 191.199353394456\n",
      "60\n",
      " acc train 188.08747740211027\n",
      " acc val 192.98829989931735\n",
      "61\n",
      " acc train 187.88521019171125\n",
      " acc val 191.1285661095197\n",
      "62\n",
      " acc train 187.3591030420482\n",
      " acc val 191.4118256864112\n",
      "63\n",
      " acc train 187.0943229952177\n",
      " acc val 192.37123100497848\n",
      "64\n",
      " acc train 187.83001617949708\n",
      " acc val 192.10452451661772\n",
      "65\n",
      " acc train 188.09295023177395\n",
      " acc val 191.45192396159698\n",
      "66\n",
      " acc train 186.90566580835971\n",
      " acc val 191.0295021248651\n",
      "67\n",
      " acc train 186.79483601934905\n",
      " acc val 191.16364453788805\n",
      "68\n",
      " acc train 186.69000813569752\n",
      " acc val 191.02390105936553\n",
      "69\n",
      " acc train 186.2250659202383\n",
      " acc val 191.88364788094822\n",
      "70\n",
      " acc train 186.1299443590726\n",
      " acc val 190.73973445113748\n",
      "71\n",
      " acc train 186.14370215910924\n",
      " acc val 192.03299094602582\n",
      "72\n",
      " acc train 186.21781917647704\n",
      " acc val 191.02695021162137\n",
      "73\n",
      " acc train 185.80654103974172\n",
      " acc val 191.14558352676093\n",
      "74\n",
      " acc train 186.09569382586733\n",
      " acc val 194.16196014121857\n",
      "75\n",
      " acc train 187.4705700186088\n",
      " acc val 191.19928167517128\n",
      "76\n",
      " acc train 185.84713242458642\n",
      " acc val 190.39483889217647\n",
      "77\n",
      " acc train 185.2217528717368\n",
      " acc val 189.61532892986318\n",
      "78\n",
      " acc train 185.33991281417553\n",
      " acc val 190.26571137643174\n",
      "79\n",
      " acc train 184.8220384339229\n",
      " acc val 191.46153571703996\n",
      "80\n",
      " acc train 184.90643255612068\n",
      " acc val 191.71857695887473\n",
      "81\n",
      " acc train 184.92194149498212\n",
      " acc val 190.31897323664424\n",
      "82\n",
      " acc train 185.55480295892875\n",
      " acc val 191.120517158526\n",
      "83\n",
      " acc train 184.61943145072786\n",
      " acc val 190.0236044275947\n",
      "84\n",
      " acc train 184.42798702967755\n",
      " acc val 190.76367663832997\n",
      "85\n",
      " acc train 184.32827128438788\n",
      " acc val 189.76877190172868\n",
      "86\n",
      " acc train 184.70804117207018\n",
      " acc val 191.5647311886338\n",
      "87\n",
      " acc train 184.4345014269598\n",
      " acc val 191.0662912390314\n",
      "88\n",
      " acc train 184.27978300861972\n",
      " acc val 191.8379643931535\n",
      "89\n",
      " acc train 185.33385507117782\n",
      " acc val 190.78051543018415\n",
      "90\n",
      " acc train 185.89788727880384\n",
      " acc val 192.40616664189918\n",
      "91\n",
      " acc train 184.16070134513663\n",
      " acc val 189.62191347204177\n",
      "92\n",
      " acc train 184.0604695289931\n",
      " acc val 190.10674691422633\n",
      "93\n",
      " acc train 183.55950179916573\n",
      " acc val 189.44553983802678\n",
      "94\n",
      " acc train 183.31202604943678\n",
      " acc val 188.76985636998387\n",
      "95\n",
      " acc train 183.35400986662802\n",
      " acc val 189.3662896649842\n",
      "96\n",
      " acc train 183.1358292816651\n",
      " acc val 190.23412901952963\n",
      "97\n",
      " acc train 182.8306642591032\n",
      " acc val 189.9712541396779\n",
      "98\n",
      " acc train 183.02262645516586\n",
      " acc val 190.07853086501171\n",
      "99\n",
      " acc train 183.17665822129115\n",
      " acc val 192.775518968378\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(epoch)\n",
    "    vae1.train()\n",
    "    train_loss_list = []\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        x,w = batch[0], batch[1]\n",
    "        #targ = targ.unsqueeze(1)\n",
    "        loss = -vae1.compute_weighted_elbo(x, w)\n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "    \n",
    "#         l2_reg = torch.tensor(0.)\n",
    "#         for param in Dcla.parameters():\n",
    "#             l2_reg += torch.norm(param)\n",
    "#         loss += l2_lambda * l2_reg\n",
    "\n",
    "    print(\" acc train\" ,np.mean(train_loss_list))\n",
    "#     scheduler.step(np.mean(train_loss_list))\n",
    "    test_loss_list = []\n",
    "    with  torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_iterator):\n",
    "            x,w = batch[0], batch[1]\n",
    "            loss = -vae1.compute_weighted_elbo(x, w)\n",
    "            test_loss_list.append(loss.item())\n",
    "    print(\" acc val\" ,np.mean(test_loss_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1)\n",
    "b = torch.tensor(2)\n",
    "torch.min((a,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29836187",
   "metadata": {},
   "source": [
    "# kmeans Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72710e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset1.sequences.shape\n",
    "len(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5852d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(dataset1.sequences)\n",
    "kmeans.labels_\n",
    "    #array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
    "# kmeans.predict([[0, 0], [12, 3]])\n",
    "    #array([1, 0], dtype=int32)\n",
    "kmeans.cluster_centers_\n",
    "#     array([[10.,  2.],\n",
    "#            [ 1.,  2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(kmeans.labels_==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b352ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(dataset1.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_r = pca.transform(dataset1.sequences)\n",
    "colors = [\"navy\", \"turquoise\", \"darkorange\", \"red\", \"blue\"]\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], kmeans.labels_):\n",
    "    plt.scatter(\n",
    "        X_r[kmeans.labels_ == i, 0], X_r[kmeans.labels_ == i, 1], color=color, alpha=0.8, lw=lw, label=target_name\n",
    "    )\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.title(\"PCA of IRIS dataset\")\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a7cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = torch.utils.data.Subset(dataset1, kmeans.labels_==4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[:][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d810684",
   "metadata": {},
   "source": [
    "# Meta Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bce9c1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_iterator))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bff2d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(model, batch):\n",
    "    seq = batch[0]\n",
    "    weights = batch[1]\n",
    "    return -1*model.compute_weighted_elbo(seq, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e234ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae1 = VAE(21, 5, dataset1.len_protein * dataset1.q, [512, 256, 128])\n",
    "optimizer1 = optim.Adam(vae1.parameters(),weight_decay=0.01)\n",
    "Node1 = PhyloNode(vae1,\n",
    "          optimizer1, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset1, \n",
    "          tuplesize=2, \n",
    "          batch_size=l1, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name = \"196\"\n",
    "    )\n",
    "\n",
    "\n",
    "vae2 = VAE(21, 5, dataset2.len_protein * dataset2.q, [512, 256, 128])\n",
    "optimizer2 = optim.Adam(vae2.parameters(),weight_decay=0.01)\n",
    "Node2 = PhyloNode(vae2,\n",
    "          optimizer2, \n",
    "          lossfn,\n",
    "          parent=None, \n",
    "          children=[], \n",
    "          dataset = dataset2, \n",
    "          tuplesize=2, \n",
    "          batch_size=l2, \n",
    "          gammaManager = gammaManager_Independant(),\n",
    "          Name=\"486\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41feb1bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13972/433847133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreplicas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreplica_state_dicts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreplica_mean_state_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "replicas = self.replicas\n",
    "Y = self.Y\n",
    "\n",
    "replica_state_dicts = [replicas[y].state_dict() for y in range(Y)]\n",
    "replica_mean_state_dict = OrderedDict()\n",
    "\n",
    "for key in replicas[Y].state_dict().keys():\n",
    "    replica_mean_state_dict[key] = \\\n",
    "        torch.mean(torch.stack([state_dict[key] for state_dict in replica_state_dicts]), 0).detach()\n",
    "\n",
    "replicas[Y].load_state_dict(replica_mean_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7657619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "vae1 = vae1.to(device)\n",
    "vae2 = vae2.to(device)\n",
    "\n",
    "\n",
    "gamma=torch.tensor(0.1, requires_grad=True)\n",
    "optimizerGamma = optim.Adam(vaeR.parameters(),weight_decay=0.01)\n",
    "mylearningModel = copy.deepcopy(vae1)\n",
    "\n",
    "# for p in mylearningModel.parameters():\n",
    "#     p.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb0d5e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000001E0851CD580>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mylearningModel.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4965b231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder_linears): ModuleList(\n",
       "    (0): Linear(in_features=2352, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (encoder_mu): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (encoder_logsigma): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (decoder_linears): ModuleList(\n",
       "    (0): Linear(in_features=5, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=2352, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e0f1594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Parameter containing:\n",
      "tensor([[ 0.0316,  0.0246, -0.0102,  ...,  0.0235, -0.0160, -0.0120],\n",
      "        [ 0.0049, -0.0327,  0.0039,  ...,  0.0314,  0.0055, -0.0130],\n",
      "        [-0.0426, -0.0048,  0.0108,  ...,  0.0152, -0.0345,  0.0260],\n",
      "        ...,\n",
      "        [ 0.0038,  0.0261,  0.0352,  ..., -0.0382,  0.0087,  0.0032],\n",
      "        [ 0.0396,  0.0232,  0.0269,  ..., -0.0109,  0.0322, -0.0409],\n",
      "        [-0.0045, -0.0162, -0.0167,  ..., -0.0276,  0.0189,  0.0029]],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[ 0.0161,  0.0031, -0.0433,  ..., -0.0086, -0.0327,  0.0414],\n",
      "        [ 0.0336,  0.0307, -0.0148,  ...,  0.0176, -0.0044,  0.0324],\n",
      "        [-0.0125, -0.0222, -0.0311,  ..., -0.0359,  0.0257, -0.0188],\n",
      "        ...,\n",
      "        [-0.0231,  0.0213,  0.0113,  ..., -0.0319, -0.0403, -0.0168],\n",
      "        [ 0.0052,  0.0372,  0.0307,  ..., -0.0283, -0.0111,  0.0348],\n",
      "        [ 0.0155, -0.0379,  0.0376,  ...,  0.0086, -0.0198, -0.0292]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "loss tensor(341.6271, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mylearningModel = VAE(21, 5, dataset1.len_protein * dataset1.q, [512, 256, 128])\n",
    "import copy\n",
    "vae1 = vae1.to(device)\n",
    "vae2 = vae2.to(device)\n",
    "vae1.eval()\n",
    "\n",
    "gamma =torch.tensor(0.1, requires_grad=True)\n",
    "\n",
    "# mylearningModel = copy.deepcopy(vae1)\n",
    "optimizerGamma = optim.Adam(mylearningModel.parameters())\n",
    "print(\"before\",mylearningModel.encoder_linears[1].weight)\n",
    "for p1,p2,pt in zip(vae1.parameters(), vae2.parameters(), mylearningModel.parameters()):\n",
    "    pt.data = p1.data.clone().detach() + gamma * p2.data.clone().detach()\n",
    "print(\"after\",mylearningModel.encoder_linears[1].weight)\n",
    "\n",
    "loss = vae_loss(mylearningModel,batch)\n",
    "print(\"loss\", loss)\n",
    "loss.backward()\n",
    "print(gamma.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9135d23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4603e-03,  5.7922e-03, -6.8263e-03, -1.4060e-03, -4.1660e-03,\n",
       "         9.8772e-04,  8.5723e-03, -1.2746e-03,  3.4750e-03, -4.5513e-03,\n",
       "         4.0655e-04, -5.7472e-03, -7.8497e-03, -2.8538e-03,  7.4794e-03,\n",
       "        -2.0981e-04,  2.2235e-03,  1.9217e-03,  1.9003e-03,  4.8133e-03,\n",
       "        -3.0225e-03, -5.4674e-03, -7.6896e-03,  4.3949e-03,  2.3638e-03,\n",
       "        -1.2172e-03, -5.9228e-03, -2.1191e-03,  6.2634e-04,  5.8063e-04,\n",
       "         4.7241e-03, -8.0270e-03,  6.2557e-03, -8.4195e-03, -1.6472e-03,\n",
       "         6.4508e-03,  1.1457e-03, -3.9582e-03, -6.4723e-03, -7.3779e-03,\n",
       "         8.7304e-04,  4.8071e-03,  5.3020e-04, -7.1232e-04, -6.1115e-03,\n",
       "         1.7270e-03,  6.0199e-03,  4.0438e-03,  3.5270e-04,  5.2790e-03,\n",
       "        -1.8656e-03,  4.0551e-04, -6.8779e-03,  5.0000e-03, -2.9553e-03,\n",
       "         3.1306e-03,  3.7630e-03, -7.6335e-03, -2.4267e-03,  9.8965e-04,\n",
       "        -5.3717e-03,  2.4834e-03, -4.6136e-03,  7.2253e-03,  3.3006e-03,\n",
       "         3.1863e-03,  1.0567e-02,  4.1016e-03,  5.9238e-03, -4.8843e-03,\n",
       "        -9.1999e-03,  3.9368e-03,  5.3737e-03,  4.4010e-03, -3.7509e-03,\n",
       "        -9.1028e-03,  2.9723e-03,  2.7863e-04, -3.5641e-03,  2.0883e-03,\n",
       "         5.0908e-03, -6.3025e-04, -7.4971e-03, -1.2628e-03, -1.0465e-03,\n",
       "        -1.0078e-03,  1.7474e-03, -5.3188e-03,  3.3623e-03,  4.1634e-03,\n",
       "        -1.7512e-03,  4.1580e-03,  3.7472e-03, -5.4153e-03,  2.0657e-03,\n",
       "         6.3328e-03, -2.6886e-03, -1.3347e-03, -5.8166e-03, -6.0085e-04,\n",
       "        -4.6742e-03, -2.5775e-03, -1.0284e-02, -4.4805e-03,  5.9725e-06,\n",
       "        -6.3082e-03, -1.6388e-03,  1.3493e-05, -6.5050e-03, -2.2879e-03,\n",
       "         8.1556e-03,  2.1172e-03, -5.4838e-04, -8.7372e-04,  2.6865e-03,\n",
       "        -4.5675e-03, -1.6789e-03, -1.2599e-03,  4.1374e-03, -3.9866e-03,\n",
       "         3.4453e-03, -1.7813e-03, -3.1168e-03, -1.1698e-03,  6.9328e-04,\n",
       "         3.3707e-03,  3.3481e-03, -2.7173e-03,  2.7397e-03,  2.7761e-03,\n",
       "         1.6420e-03,  4.5148e-03, -5.5967e-03,  3.6467e-03,  4.3122e-04,\n",
       "         2.2394e-03,  5.7206e-03,  5.2860e-03,  6.2835e-03, -3.8767e-03,\n",
       "        -5.0065e-03,  7.5861e-03, -2.1883e-03, -7.6401e-03,  1.5974e-03,\n",
       "         2.2241e-03,  5.0949e-03, -9.7453e-04, -1.8073e-05, -2.6946e-03,\n",
       "        -1.5046e-03,  3.3479e-03, -5.1695e-04, -5.7622e-05, -5.6987e-03,\n",
       "        -6.9114e-03,  9.4026e-04, -8.8384e-03,  3.4146e-03,  3.4087e-03,\n",
       "         7.9628e-03,  1.0538e-02, -8.1186e-03, -2.3292e-04, -1.3823e-03,\n",
       "         2.9932e-03, -2.5585e-03, -3.8509e-04, -1.8884e-03,  4.8684e-03,\n",
       "        -5.9862e-03, -7.7888e-03, -2.7202e-03, -2.1516e-04, -1.6271e-03,\n",
       "         2.3413e-03, -5.3488e-04, -3.9527e-03,  1.0618e-02, -1.0090e-02,\n",
       "        -3.1837e-03, -5.7629e-03, -4.2893e-03,  3.1764e-03, -4.7642e-04,\n",
       "        -8.5648e-04,  1.2116e-02, -2.5025e-04,  3.0743e-03,  1.9871e-03,\n",
       "        -3.1478e-03, -5.0115e-03,  7.6971e-05, -3.0399e-04,  4.7965e-03,\n",
       "        -3.3158e-03,  7.5624e-04, -1.4720e-03, -1.8277e-03, -2.3583e-04,\n",
       "        -1.1121e-02,  8.1868e-03, -7.4028e-04, -3.3195e-03, -1.4995e-03,\n",
       "        -2.8445e-03, -3.5686e-03,  3.1185e-03,  4.1929e-03, -6.2557e-04,\n",
       "        -3.6258e-03, -1.0940e-04, -4.5430e-04, -6.1077e-03, -7.6579e-04,\n",
       "         5.0543e-04, -6.2420e-04,  2.6258e-03,  4.8444e-03,  1.2416e-03,\n",
       "        -1.3900e-03,  6.6990e-03, -1.7498e-03,  5.6968e-03, -4.4359e-03,\n",
       "         5.7608e-03, -2.6453e-03, -3.3536e-03,  4.7394e-03,  5.5688e-04,\n",
       "        -1.9303e-05, -6.0352e-03, -9.6108e-03,  1.4096e-03, -1.9635e-03,\n",
       "         3.5219e-03,  1.9536e-03, -7.9288e-03, -4.6821e-03, -6.7998e-03,\n",
       "         4.9615e-03,  4.2079e-03, -7.7481e-03,  2.9840e-03,  7.0572e-04,\n",
       "        -7.6495e-04,  7.6330e-04,  5.3114e-03,  1.6502e-03,  2.7498e-03,\n",
       "        -9.2936e-03,  4.2099e-03,  2.2426e-03, -1.0313e-03,  6.0488e-03,\n",
       "        -9.4156e-03, -2.2203e-03, -4.3097e-03, -8.7157e-03,  3.2252e-03,\n",
       "         2.8335e-04,  1.4129e-03, -4.5957e-04, -3.9410e-03, -7.2959e-03,\n",
       "         7.0525e-03, -1.2684e-03, -3.1471e-03, -3.8117e-03,  2.3628e-03,\n",
       "         6.7082e-04, -3.5980e-03, -1.4207e-04, -2.0125e-03, -1.0322e-03,\n",
       "         4.5977e-04,  5.6006e-03,  2.0598e-03,  5.6370e-03,  3.6935e-03,\n",
       "        -2.5301e-03, -5.2451e-03, -1.3721e-04,  2.1423e-03, -4.1412e-03,\n",
       "         4.0927e-03,  1.3038e-03,  2.0024e-03, -4.6383e-03, -8.5882e-03,\n",
       "         4.3419e-03, -4.7838e-03,  6.1657e-04,  1.1662e-03,  2.1942e-03,\n",
       "        -2.1247e-04, -3.7510e-03,  4.8941e-03,  1.7748e-03,  2.5628e-05,\n",
       "         6.9318e-04,  1.2453e-03,  7.0276e-03, -1.0001e-03,  2.6384e-03,\n",
       "        -1.8669e-03,  1.9706e-03, -3.6530e-03,  1.9257e-03, -1.3940e-03,\n",
       "         7.3298e-04, -5.7461e-03,  2.3844e-04,  1.8307e-03, -3.2425e-03,\n",
       "        -1.1101e-03, -3.7286e-03,  5.5808e-03,  4.2477e-04,  5.8431e-03,\n",
       "        -3.4027e-03,  6.2427e-03,  7.3747e-05,  2.2922e-03,  2.2546e-03,\n",
       "         4.4519e-03,  4.7329e-03, -5.0921e-03, -5.4496e-03,  4.8872e-03,\n",
       "         3.7757e-03,  7.7382e-03, -3.5419e-03,  6.4273e-04,  4.5083e-03,\n",
       "        -9.0422e-04, -8.1945e-03, -4.4789e-03,  5.0180e-03, -3.9987e-03,\n",
       "         8.3142e-04, -4.9611e-03, -2.4459e-03, -2.6242e-03, -4.4282e-03,\n",
       "        -3.9168e-03, -2.3480e-03, -7.7027e-03, -1.3231e-03, -3.9761e-03,\n",
       "         1.4086e-03,  2.6582e-03,  1.1392e-02, -1.1306e-03,  3.2996e-03,\n",
       "        -5.1376e-03, -1.9650e-03,  7.2256e-03,  3.6675e-03, -3.6459e-03,\n",
       "         6.2060e-03,  4.7416e-03,  1.5782e-04,  3.5917e-03,  2.3560e-03,\n",
       "        -3.2988e-03,  1.2302e-03,  3.5968e-03, -2.6769e-03, -5.2990e-03,\n",
       "         1.0115e-03, -2.1213e-03,  1.1246e-02, -3.8509e-03, -3.1196e-03,\n",
       "        -3.7480e-03,  2.0553e-03,  3.4249e-03,  1.7058e-03, -3.7251e-03,\n",
       "        -9.7863e-03, -7.8102e-03, -6.4253e-03,  1.8351e-03,  6.4128e-03,\n",
       "         4.7043e-03,  2.2198e-04, -1.6661e-03,  4.2350e-04, -1.8221e-03,\n",
       "         5.7674e-03, -3.9945e-03, -7.2962e-05, -8.4853e-03, -3.9775e-03,\n",
       "         5.6905e-03,  2.7044e-03, -2.5184e-03,  2.9312e-03,  8.8784e-05,\n",
       "        -3.6718e-03,  4.4118e-03, -1.3466e-03, -4.4896e-03,  2.4085e-03,\n",
       "         6.8968e-03, -7.8712e-03,  1.8009e-03, -2.7155e-04, -4.7283e-03,\n",
       "        -3.7090e-03,  1.3652e-03, -2.7547e-03,  1.3521e-03, -3.9193e-03,\n",
       "        -1.6308e-03, -6.7625e-03,  1.2051e-03,  3.3175e-03,  1.6475e-03,\n",
       "         2.0117e-03,  2.7612e-03, -3.5218e-03,  8.1794e-04, -2.8156e-03,\n",
       "         3.9510e-03, -2.6545e-03, -7.2837e-03, -2.4526e-03,  4.6731e-04,\n",
       "        -1.3731e-03, -4.8965e-03, -7.7326e-04,  6.3766e-03,  2.3552e-03,\n",
       "         2.3538e-03, -7.2239e-03, -1.2597e-03, -6.1420e-03, -1.7536e-03,\n",
       "         2.0059e-04, -3.1091e-03,  6.7162e-04,  3.6643e-03, -3.8558e-03,\n",
       "        -5.7921e-03,  5.1026e-03,  1.9067e-03,  4.2900e-03,  1.2484e-02,\n",
       "        -3.1972e-03,  2.0373e-04, -4.3298e-05,  2.8489e-03,  6.6308e-03,\n",
       "         3.5242e-03, -4.7403e-04,  1.6738e-03, -4.1831e-03,  2.5148e-03,\n",
       "         4.2595e-03, -5.7057e-04,  4.1924e-03, -3.3140e-03, -1.9922e-03,\n",
       "         5.4966e-04, -4.7455e-03,  2.5685e-03, -6.3698e-03,  8.1480e-03,\n",
       "         6.2909e-03, -5.7416e-03,  6.4658e-04,  8.5628e-03, -5.3810e-03,\n",
       "        -2.2244e-03,  6.3483e-03,  5.4523e-03,  3.2696e-03, -2.2320e-03,\n",
       "        -6.6206e-04,  5.7851e-03,  1.3691e-02, -3.4016e-03, -8.0943e-03,\n",
       "         2.0896e-03, -4.5654e-03, -1.1399e-03,  3.7266e-03,  6.9986e-03,\n",
       "         4.1407e-03, -3.1711e-03,  7.7162e-03,  5.8460e-03, -6.7281e-04,\n",
       "        -2.7006e-03, -5.0245e-03, -2.8144e-03, -3.8249e-03, -5.7193e-03,\n",
       "        -4.2124e-04, -2.3130e-03,  2.1636e-03,  2.8651e-04, -2.2919e-03,\n",
       "         9.1023e-04, -7.6840e-03,  2.4506e-04, -8.4025e-04, -5.3075e-03,\n",
       "        -3.3970e-03, -6.8552e-03], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mylearningModel.parameters())[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594c2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(vae2.parameters())\n",
    "params[1].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d131c11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder_linears): ModuleList(\n",
      "    (0): Linear(in_features=2352, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (encoder_mu): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (encoder_logsigma): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (decoder_linears): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): Linear(in_features=128, out_features=2352, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1568/2334365480.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizerGamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmylearningModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmylearningModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_linears\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_unused\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Barth\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "optimizerGamma.zero_grad()\n",
    "print(mylearningModel)\n",
    "print(torch.autograd.grad(loss, mylearningModel.encoder_linears[1].weight, retain_graph=True, create_graph=True, only_inputs=True, allow_unused=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d150e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gammaGradient(model1, model2, modelRobot, batch, gamma):\n",
    "    modelRobot.zero_grad()\n",
    "    for p1,p2,pt in zip(model1.parameters(), model2.parameters(), mylearningModel.parameters()):\n",
    "        pt.data = p1.data.clone().detach() + gamma * (p2.data.clone().detach() - p1.data.clone().detach())\n",
    "    loss = vae_loss(modelRobot,batch)\n",
    "    loss.backward()\n",
    "    paramsmodelRobot = list(modelRobot.parameters())\n",
    "    paramsmodel1 = list(model1.parameters())\n",
    "    paramsmodel2 = list(model2.parameters())\n",
    "    grad = 0\n",
    "    for i in range(len(paramsmodel1)):\n",
    "        dif = (paramsmodel2[i].data - paramsmodel1[i].data)*paramsmodelRobot[i].grad\n",
    "        grad += torch.sum(dif)\n",
    "    return grad\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b7b3aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5507, device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammaGradient(vae1, vae2, mylearningModel, batch, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3ebcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mylearningModel.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af87223",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OrderedDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13972/3611765417.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'OrderedDict' is not defined"
     ]
    }
   ],
   "source": [
    "grad = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "902592c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(341.7444, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vae1.eval()\n",
    "loss = vae_loss(vae1,batch)\n",
    "print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39dd1477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.5299e-04, -4.1638e-04,  5.0943e-04,  ..., -4.3712e-04,\n",
       "         -5.2714e-04,  2.2105e-05],\n",
       "        [ 2.7423e-04,  1.8226e-04, -9.2665e-05,  ...,  3.7737e-04,\n",
       "         -2.1523e-05,  1.3462e-04],\n",
       "        [-3.5861e-04, -3.1159e-04,  9.2236e-04,  ..., -5.1846e-04,\n",
       "          1.7147e-04, -2.4603e-04],\n",
       "        ...,\n",
       "        [ 8.9274e-05,  3.3852e-04,  6.9726e-04,  ...,  1.3256e-04,\n",
       "          1.7706e-04,  6.5539e-05],\n",
       "        [ 2.3475e-04,  5.4092e-04, -1.4460e-03,  ..., -6.9047e-04,\n",
       "         -1.0559e-03,  9.4670e-04],\n",
       "        [-6.8411e-04, -2.9299e-04,  6.4204e-04,  ..., -1.0027e-03,\n",
       "         -9.2636e-04,  1.6536e-04]], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylearningModel.encoder_linears[1].weight.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e94e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pt in mylearningModel.parameters():\n",
    "    pt.data = 0*pt.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f194c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0., -0., -0.,  ..., -0., 0., 0.],\n",
      "        [-0., -0., 0.,  ..., -0., -0., -0.],\n",
      "        [0., -0., 0.,  ..., -0., -0., 0.],\n",
      "        ...,\n",
      "        [-0., -0., 0.,  ..., -0., -0., 0.],\n",
      "        [-0., 0., -0.,  ..., 0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., 0., -0., 0.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0., 0., 0., -0., 0., -0., 0., 0., -0., -0., -0., 0., 0., 0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0.,\n",
      "        -0., -0., 0., -0., -0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., -0.,\n",
      "        -0., 0., -0., 0., 0., -0., -0., -0., -0., -0., -0., 0., -0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., -0.,\n",
      "        0., -0., -0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., -0., -0., 0.,\n",
      "        -0., 0., 0., 0., -0., 0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., -0., 0., 0., -0., 0.,\n",
      "        0., 0., -0., -0., 0., -0., 0., 0., 0., -0., -0., -0., 0., 0., -0., -0., -0., -0., -0., 0., 0., 0., -0., -0.,\n",
      "        0., 0., 0., 0., 0., -0., -0., 0., -0., -0., 0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., -0., -0., 0.,\n",
      "        -0., 0., -0., -0., 0., -0., -0., 0., -0., -0., -0., 0., 0., 0., 0., 0., -0., 0., 0., 0., 0., -0., -0., 0.,\n",
      "        -0., 0., -0., 0., 0., -0., 0., -0., -0., 0., -0., 0., 0., 0., -0., 0., -0., -0., 0., -0., 0., 0., -0., -0.,\n",
      "        0., 0., -0., -0., -0., -0., -0., 0., 0., -0., 0., -0., -0., 0., -0., -0., 0., -0., 0., 0., -0., 0., -0., -0.,\n",
      "        0., -0., 0., 0., 0., -0., -0., -0., 0., -0., 0., 0., -0., 0., 0., -0., -0., -0., 0., 0., -0., 0., -0., 0.,\n",
      "        0., -0., -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., -0., 0., 0., -0., -0.,\n",
      "        -0., 0., -0., -0., -0., 0., 0., -0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., -0., 0., -0., 0., 0., -0.,\n",
      "        0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., -0., -0., -0., -0., -0., 0., 0., -0.,\n",
      "        0., 0., -0., -0., -0., -0., 0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0.,\n",
      "        0., -0., -0., -0., -0., 0., -0., -0., 0., 0., -0., -0., -0., 0., -0., 0., 0., 0., 0., -0., -0., -0., 0., -0.,\n",
      "        -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., -0., -0., -0., -0., -0., -0., 0., -0., -0., -0., 0., -0.,\n",
      "        0., -0., -0., 0., -0., -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., 0.,\n",
      "        -0., 0., 0., -0., -0., 0., -0., 0., 0., -0., 0., -0., 0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., -0.,\n",
      "        -0., 0., 0., 0., 0., -0., 0., -0., -0., 0., -0., 0., 0., 0., -0., -0., -0., 0., 0., -0., 0., -0., -0., 0.,\n",
      "        -0., 0., -0., -0., 0., -0., 0., -0., -0., 0., -0., 0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., -0.,\n",
      "        -0., -0., -0., 0., 0., -0., -0., 0.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        [0., -0., 0.,  ..., -0., -0., 0.],\n",
      "        [0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        ...,\n",
      "        [-0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., -0., -0.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., -0., 0., -0., -0., -0., 0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., -0., 0., -0.,\n",
      "        0., 0., 0., -0., 0., -0., -0., -0., 0., 0., 0., -0., 0., 0., -0., -0., -0., 0., -0., 0., 0., 0., -0., 0.,\n",
      "        -0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., -0., -0., -0., -0., -0., -0., 0., -0.,\n",
      "        0., -0., -0., -0., 0., -0., 0., -0., 0., -0., 0., -0., 0., -0., -0., -0., 0., 0., 0., -0., -0., -0., -0., -0.,\n",
      "        -0., 0., -0., 0., -0., -0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0., -0., -0., 0.,\n",
      "        -0., 0., -0., 0., -0., 0., 0., -0., 0., -0., 0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., -0., 0., -0.,\n",
      "        -0., -0., -0., -0., 0., -0., 0., 0., -0., 0., 0., 0., -0., -0., 0., -0., -0., 0., -0., -0., 0., 0., -0., -0.,\n",
      "        -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., -0., 0., 0.,\n",
      "        -0., -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., -0.,\n",
      "        0., 0., 0., -0., -0., 0., -0., 0., -0., -0., -0., 0., -0., -0., -0., -0., 0., -0., -0., 0., -0., -0., 0., 0.,\n",
      "        0., -0., -0., -0., -0., -0., -0., 0., 0., 0., -0., 0., -0., 0., 0., -0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0., -0., 0.,  ..., -0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., 0., 0., -0.],\n",
      "        [-0., -0., -0.,  ..., 0., -0., 0.],\n",
      "        ...,\n",
      "        [-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., 0., -0.],\n",
      "        [-0., -0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., -0., 0., -0., -0., 0., -0., -0., -0., -0., -0., 0., -0., 0., -0., -0., 0., 0., -0., 0., -0., -0.,\n",
      "        -0., 0., 0., -0., -0., 0., 0., 0., -0., -0., -0., 0., 0., 0., 0., -0., 0., 0., 0., 0., 0., -0., -0., -0.,\n",
      "        0., 0., -0., 0., 0., -0., -0., -0., -0., 0., -0., -0., -0., -0., 0., 0., 0., -0., -0., -0., 0., -0., -0., -0.,\n",
      "        -0., 0., 0., -0., -0., 0., 0., 0., 0., -0., 0., -0., 0., 0., -0., 0., -0., -0., -0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., -0., 0., 0., -0., -0., 0., 0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0., 0., -0.,\n",
      "        0., 0., -0., -0., -0., -0., -0., -0.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0., -0., 0., -0., 0., 0., -0., 0., -0., -0., -0., 0., -0., 0., -0., 0., 0., 0., -0., -0., 0., -0., -0., -0.,\n",
      "         -0., 0., 0., 0., -0., -0., -0., -0., 0., -0., -0., 0., 0., 0., 0., -0., -0., 0., -0., 0., 0., -0., -0., -0.,\n",
      "         0., 0., -0., 0., 0., -0., -0., 0., -0., 0., 0., -0., -0., 0., 0., -0., 0., -0., 0., -0., -0., 0., -0., 0.,\n",
      "         0., -0., -0., -0., -0., -0., -0., 0., -0., 0., 0., 0., -0., -0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0.,\n",
      "         -0., 0., -0., -0., 0., 0., 0., -0., -0., -0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0., 0., -0., -0., -0.,\n",
      "         -0., 0., -0., 0., -0., 0., -0., 0.],\n",
      "        [-0., 0., -0., 0., 0., 0., -0., 0., 0., -0., 0., -0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., -0., -0.,\n",
      "         -0., 0., 0., 0., 0., 0., -0., -0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., -0., 0., -0., -0., -0., 0.,\n",
      "         -0., -0., 0., -0., -0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., -0., 0., 0., 0., -0., -0., -0., -0.,\n",
      "         0., 0., -0., -0., -0., 0., -0., -0., 0., 0., 0., 0., 0., 0., 0., -0., -0., 0., -0., 0., -0., -0., -0., 0.,\n",
      "         -0., 0., -0., -0., 0., 0., -0., 0., -0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0., -0., -0.,\n",
      "         -0., 0., 0., -0., 0., -0., -0., 0.],\n",
      "        [0., -0., 0., 0., -0., -0., 0., 0., 0., 0., 0., -0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., -0., 0.,\n",
      "         -0., 0., 0., -0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0., 0., -0., 0., 0., -0., -0., -0., -0., -0., 0.,\n",
      "         0., -0., -0., -0., 0., -0., 0., -0., -0., -0., -0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0., 0., 0., 0.,\n",
      "         -0., 0., 0., -0., -0., 0., -0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., 0., -0., -0., -0., -0.,\n",
      "         -0., 0., 0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., -0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0.,\n",
      "         0., -0., 0., 0., -0., 0., -0., 0.],\n",
      "        [-0., -0., 0., -0., 0., -0., 0., -0., 0., 0., -0., -0., 0., 0., -0., -0., 0., -0., -0., 0., -0., -0., -0., -0.,\n",
      "         0., -0., 0., 0., 0., 0., -0., -0., 0., -0., 0., -0., -0., -0., 0., 0., 0., -0., 0., -0., -0., -0., 0., -0.,\n",
      "         0., 0., 0., -0., 0., 0., -0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., 0., 0.,\n",
      "         -0., -0., 0., 0., 0., 0., -0., 0., 0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0., 0., -0., 0., 0., 0.,\n",
      "         0., 0., 0., -0., -0., -0., 0., -0., 0., 0., -0., -0., -0., 0., 0., -0., 0., -0., -0., 0., -0., -0., 0., -0.,\n",
      "         -0., 0., 0., -0., -0., 0., 0., 0.],\n",
      "        [0., -0., 0., -0., 0., -0., 0., -0., 0., 0., 0., -0., -0., 0., -0., -0., 0., 0., 0., 0., -0., -0., 0., 0.,\n",
      "         0., 0., 0., -0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., 0., -0., 0., 0., -0., 0., -0., 0., -0., -0.,\n",
      "         0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., 0., -0., -0., 0., 0., -0., -0., 0., -0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0.,\n",
      "         -0., 0., -0., 0., -0., -0., 0., -0., -0., 0., 0., 0., 0., -0., 0., 0., -0., -0., -0., 0., 0., -0., -0., -0.,\n",
      "         -0., 0., -0., 0., -0., -0., 0., 0.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., -0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., -0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., 0.,\n",
      "         -0., -0., -0., -0., 0., -0., 0., 0., -0., 0., -0., -0., -0., 0., -0., -0., 0., -0., -0., 0., 0., 0., -0., -0.,\n",
      "         -0., 0., -0., -0., -0., -0., 0., -0., 0., 0., -0., -0., 0., -0., -0., -0., 0., -0., -0., 0., -0., 0., 0., -0.,\n",
      "         -0., 0., 0., 0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0.,\n",
      "         -0., 0., -0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., 0., -0., 0., -0., 0., 0., -0., -0., -0., -0., 0.,\n",
      "         -0., 0., -0., 0., 0., -0., 0., -0.],\n",
      "        [0., 0., 0., -0., -0., -0., -0., 0., 0., -0., -0., 0., -0., -0., 0., -0., 0., -0., -0., -0., -0., -0., 0., 0.,\n",
      "         -0., 0., -0., 0., 0., -0., -0., 0., 0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., 0., 0., 0., -0., -0.,\n",
      "         0., -0., 0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., -0.,\n",
      "         -0., -0., 0., -0., -0., -0., -0., 0., 0., -0., 0., 0., -0., -0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0.,\n",
      "         -0., -0., -0., -0., -0., 0., -0., 0., -0., -0., 0., -0., -0., 0., 0., -0., 0., -0., -0., 0., 0., 0., -0., -0.,\n",
      "         0., -0., 0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., 0., 0., 0., 0., -0., 0., -0., 0., -0., -0., -0., 0., -0., -0., 0., 0., -0., 0., 0., 0., 0., -0.,\n",
      "         -0., 0., 0., -0., -0., 0., 0., -0., 0., 0., 0., -0., -0., -0., -0., 0., 0., 0., 0., 0., -0., -0., 0., -0.,\n",
      "         0., -0., -0., 0., -0., 0., 0., -0., -0., 0., -0., -0., -0., 0., 0., -0., 0., -0., 0., 0., 0., -0., -0., -0.,\n",
      "         0., 0., -0., 0., 0., 0., 0., 0., 0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., 0., 0., 0., 0.,\n",
      "         -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., 0., 0., -0., 0., 0.,\n",
      "         -0., -0., -0., -0., 0., 0., 0., 0.],\n",
      "        [-0., -0., 0., -0., -0., 0., -0., -0., -0., 0., 0., -0., 0., -0., 0., -0., -0., 0., 0., 0., -0., 0., -0., -0.,\n",
      "         -0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., -0., 0., -0., -0., 0., -0., -0.,\n",
      "         0., -0., 0., 0., 0., -0., -0., 0., -0., 0., 0., 0., -0., -0., 0., 0., -0., -0., -0., 0., 0., -0., -0., 0.,\n",
      "         -0., 0., -0., -0., 0., -0., 0., -0., 0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0., 0.,\n",
      "         0., 0., 0., -0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., 0., -0., -0., -0., -0., 0., -0., -0., -0., 0.,\n",
      "         0., 0., 0., 0., -0., 0., -0., 0.],\n",
      "        [0., 0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., 0., 0.,\n",
      "         0., -0., 0., -0., -0., 0., 0., 0., 0., -0., -0., 0., 0., 0., 0., 0., -0., 0., 0., -0., -0., -0., -0., -0.,\n",
      "         -0., 0., -0., 0., -0., 0., -0., -0., -0., -0., 0., -0., -0., -0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0.,\n",
      "         -0., -0., -0., 0., -0., -0., 0., -0., 0., 0., -0., 0., -0., 0., -0., 0., -0., 0., 0., 0., 0., -0., 0., -0.,\n",
      "         0., 0., -0., 0., -0., -0., 0., 0., -0., 0., -0., -0., 0., -0., 0., 0., 0., 0., 0., 0., -0., -0., 0., -0.,\n",
      "         -0., 0., -0., 0., 0., -0., 0., 0.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., -0., -0., 0., -0.], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., -0., 0., -0., -0.],\n",
      "        [0., -0., 0., 0., 0.],\n",
      "        [-0., 0., -0., 0., -0.],\n",
      "        ...,\n",
      "        [-0., 0., -0., 0., 0.],\n",
      "        [0., -0., 0., 0., -0.],\n",
      "        [0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., -0., 0., 0., -0., -0., -0., 0., 0., -0., -0., 0., 0., -0., -0., 0., -0., -0., 0., 0., 0., -0., -0., 0.,\n",
      "        -0., -0., -0., 0., -0., 0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0., -0., -0., 0., -0., 0., -0., 0., 0.,\n",
      "        0., -0., -0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., -0., -0., -0., -0., 0., -0., -0., -0., -0.,\n",
      "        0., -0., -0., 0., -0., 0., 0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0., 0., -0., 0., -0.,\n",
      "        -0., 0., -0., 0., 0., -0., -0., 0., 0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., -0., 0., 0., -0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., 0., -0., 0., -0., 0.,\n",
      "        0., -0., -0., 0., 0., -0., 0., 0., -0., 0., -0., -0., -0., 0., 0., -0., -0., -0., 0., -0., -0., -0., 0., -0.,\n",
      "        -0., -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., -0., 0., -0., -0., -0., 0., 0., 0., -0., 0., 0., 0., 0.,\n",
      "        0., -0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., 0., -0., 0., -0., 0., 0., 0., 0., -0., 0., -0., -0.,\n",
      "        0., 0., -0., 0., -0., 0., -0., -0., 0., 0., -0., 0., -0., 0., 0., -0., 0., -0., 0., -0., -0., 0., 0., 0.,\n",
      "        -0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., 0., -0., -0., 0., 0., 0., 0., 0., 0., -0., 0., 0., 0.,\n",
      "        -0., 0., -0., 0., -0., -0., 0., -0., -0., -0., -0., 0., 0., 0., 0., -0., 0., -0., -0., 0., -0., -0., -0., 0.,\n",
      "        -0., -0., -0., 0., 0., 0., -0., -0., 0., -0., 0., 0., -0., -0., 0., -0., -0., 0., -0., -0., -0., -0., 0., -0.,\n",
      "        -0., 0., 0., 0., 0., 0., -0., -0., 0., -0., -0., -0., 0., 0., 0., 0., 0., -0., 0., 0., -0., 0., -0., 0.,\n",
      "        0., -0., -0., -0., 0., -0., -0., 0., -0., -0., -0., -0., -0., -0., 0., 0., -0., -0., -0., -0., 0., -0., 0., -0.,\n",
      "        0., 0., 0., 0., 0., -0., -0., -0., 0., 0., -0., 0., -0., 0., -0., 0., 0., -0., 0., -0., -0., 0., 0., -0.,\n",
      "        0., -0., -0., 0., 0., -0., -0., 0., 0., 0., -0., -0., -0., 0., -0., 0., -0., -0., -0., 0., -0., 0., -0., 0.,\n",
      "        -0., 0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0., -0., -0., -0., -0., -0., 0., -0., 0., -0., -0.,\n",
      "        0., -0., -0., 0., -0., 0., -0., 0., 0., 0., 0., -0., 0., 0., -0., 0., 0., -0., -0., -0., 0., 0., 0., 0.,\n",
      "        -0., 0., -0., 0., 0., -0., -0., 0., 0., 0., -0., -0., 0., -0., -0., -0., 0., -0., 0., -0., 0., 0., -0., 0.,\n",
      "        -0., -0., 0., 0., 0., -0., 0., -0., -0., -0., -0., -0., 0., -0., -0., -0., -0., -0., -0., 0., 0., 0., -0., -0.,\n",
      "        0., 0., 0., -0., 0., 0., -0., -0.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0., -0., -0.,  ..., 0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., -0., -0., -0.],\n",
      "        [-0., -0., 0.,  ..., -0., -0., 0.],\n",
      "        ...,\n",
      "        [-0., 0., 0.,  ..., 0., -0., -0.],\n",
      "        [0., 0., -0.,  ..., 0., -0., 0.],\n",
      "        [0., -0., -0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0., 0., -0., 0., -0., 0., 0., 0., 0., 0., -0., 0., -0., -0., -0., -0., -0., -0., 0., 0., -0., 0., 0., -0.,\n",
      "        -0., -0., 0., 0., 0., -0., 0., -0., 0., -0., -0., 0., 0., -0., -0., 0., 0., -0., 0., -0., -0., 0., 0., -0.,\n",
      "        0., -0., 0., -0., 0., 0., 0., 0., 0., 0., -0., 0., -0., 0., -0., -0., -0., 0., -0., -0., 0., -0., -0., -0.,\n",
      "        -0., 0., 0., 0., -0., -0., -0., 0., -0., -0., -0., -0., -0., -0., 0., -0., -0., -0., 0., -0., 0., -0., -0., 0.,\n",
      "        -0., -0., -0., -0., -0., 0., -0., -0., -0., -0., 0., 0., 0., -0., -0., -0., 0., 0., 0., 0., 0., 0., -0., -0.,\n",
      "        0., -0., -0., 0., 0., -0., -0., 0., 0., 0., -0., 0., -0., -0., -0., -0., 0., -0., 0., -0., 0., 0., 0., -0.,\n",
      "        -0., -0., -0., -0., -0., 0., 0., -0., 0., -0., -0., -0., 0., -0., 0., 0., -0., -0., -0., -0., -0., -0., -0., -0.,\n",
      "        -0., -0., 0., -0., -0., 0., -0., -0., 0., 0., -0., 0., -0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., 0.,\n",
      "        -0., -0., -0., 0., 0., -0., -0., 0., 0., 0., 0., -0., -0., -0., 0., 0., 0., -0., 0., -0., 0., 0., 0., -0.,\n",
      "        0., 0., 0., -0., -0., -0., -0., 0., 0., 0., -0., 0., -0., 0., -0., 0., -0., 0., -0., -0., 0., -0., -0., -0.,\n",
      "        0., -0., 0., 0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., 0., -0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0., -0., 0.,  ..., 0., -0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., -0., 0.],\n",
      "        [-0., -0., -0.,  ..., 0., -0., -0.],\n",
      "        ...,\n",
      "        [-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "        [-0., 0., -0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., -0.]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., -0., -0., 0., 0., -0., -0., -0., 0., -0., -0., 0., 0., 0., 0., 0., 0., 0., -0., 0., -0., -0., -0., 0.,\n",
      "        0., 0., -0., 0., 0., 0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., -0., -0., -0., -0., -0., 0., -0., 0.,\n",
      "        -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0., 0., -0., -0., 0., 0., 0., 0., -0., -0., 0., 0., -0., -0.,\n",
      "        -0., -0., -0., -0., -0., 0., 0., 0., -0., 0., -0., -0., 0., -0., 0., -0., -0., 0., -0., 0., 0., -0., 0., 0.,\n",
      "        -0., -0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., 0., -0., -0., 0., 0., 0., -0., -0., -0., -0., 0., 0.,\n",
      "        0., 0., 0., -0., -0., 0., -0., 0.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0., -0., 0.,  ..., -0., 0., -0.],\n",
      "        [0., 0., -0.,  ..., 0., -0., 0.],\n",
      "        [-0., 0., 0.,  ..., 0., 0., -0.],\n",
      "        ...,\n",
      "        [0., -0., 0.,  ..., -0., 0., -0.],\n",
      "        [0., 0., -0.,  ..., -0., 0., 0.],\n",
      "        [-0., -0., -0.,  ..., -0., -0., -0.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0., -0., -0.,  ..., 0., 0., -0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for pt in mylearningModel.parameters():\n",
    "    print(pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c608544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f982df13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(341.8337, device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04769ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9effa97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(gamma.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440f5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gammaManager_auto(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(gammaManager_Independant, self).__init__()\n",
    "        self.gammaParents = torch.tensor(0.0)\n",
    "        self.gammaChildren = torch.tensor(0.0)\n",
    "        self.timestep = torch.tensor(0.0)\n",
    "        \n",
    "    def composeLoss(self, Node):\n",
    "        return Node.loss + self.gammaParents * Node.coupling_loss_Parents + self.gammaChildren * Node.coupling_loss_Children\n",
    "    \n",
    "    def updateGamma(self,Node):\n",
    "            self.timestep +=1\n",
    "    def reinitGamma(self, Node):\n",
    "        self.gammaParents = torch.tensor(0.0).to(self.gammaParents.device)\n",
    "        self.gammaChildren = torch.tensor(0.0).to(self.gammaChildren.device)\n",
    "    def to_(self,device):\n",
    "        self.gammaParents = self.gammaParents.to(device)\n",
    "        self.gammaChildren = self.gammaChildren.to(device)\n",
    "        self.timestep = self.timestep.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5142a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb459876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a1352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e72c6051",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3729777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys, os\n",
    "sys.path.append('C:\\\\Users\\\\bartm\\\\Documents\\\\These\\\\phyloreplica\\\\src')\n",
    "from PhyloDataset import *\n",
    "from PhyloTrees import *\n",
    "\n",
    "from vae import *\n",
    "from GProT import *\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datapath1 = \"../data/PF00072/PF00072_rp15_has_PF00196.faa\"\n",
    "datapath2 = \"../data/PF00072/PF00072_rp15_has_PF00486.faa\"\n",
    "datapath3 = \"../data/PF00072/PF00072_rp15_has_PF00512.faa\"\n",
    "datapath4 = \"data/PF00072/PF00072_rp15_has_PF00158.faa\"\n",
    "datapath5 = \"data/PF00072/PF00072_rp15_has_PF00990.faa\"\n",
    "datapath6 = \"data/PF00072/PF00072_rp15_has_PF01339.faa\"\n",
    "datapath7 = \"data/PF00072/PF00072_rp15_has_PF04397.faa\"\n",
    "datapath8 = \"data/PF00072/PF00072_rp15_has_PF12833.faa\"\n",
    "\n",
    "lossfn = vae_loss\n",
    "\n",
    "dataset1 = MSA(datapath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4356f5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d431544",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(21, 112)\n",
    "co = GPT1Config(21, 112)\n",
    "co.n_layer = 3\n",
    "co.n_head = 7\n",
    "co.n_embd = 50*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daa768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9ddfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac26177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 21])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a09289",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = GPT(co, dataset1.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c659df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(dataset1, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1025c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "152dc4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 112, 21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = batch[0]\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ca7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "913932bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 112, 21])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b43a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3c539fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e768936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "inp = batch[0].reshape(-1, batch[0].shape[2])\n",
    "tar = batch[0].max(dim=2)[1].flatten()\n",
    "F.cross_entropy(inp*10, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a86c4ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17,  9, 17,  ...,  0, 17, 14],\n",
       "        [ 7, 17,  7,  ...,  0, 17, 14],\n",
       "        [17,  0,  7,  ...,  0,  7, 14],\n",
       "        ...,\n",
       "        [17,  9, 17,  ...,  0,  7, 14],\n",
       "        [17,  7, 17,  ..., 15,  7, 20],\n",
       "        [ 7,  9,  7,  ...,  6,  7, 20]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].max(dim=2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17453c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = batch[0].reshape(-1, 21)\n",
    "tar = batch[0].max(dim=2)[1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e398afb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53d8531e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3f70640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1232)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(inp[0,:].unsqueeze(0), tar[0].unsqueeze(0), weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbc2f4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(inp[0,:].unsqueeze(0)*10, tar[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb38aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
